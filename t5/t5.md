## **Exercise 1 (15 minutes): Testing Perceptron Efficiency on Linearly and Non-Linearly Separable Data**

**Objective**: Use a pre-implemented perceptron to test its performance on both linearly separable and non-linearly separable datasets.

1. **Linearly Separable Dataset**
   - Generate 2D points with labels: 1 if \( y > x \), else 0.
   - Train the perceptron on this dataset.
   - Evaluate and print the training accuracy.

2. **Non-Linearly Separable Dataset**
   - Create an XOR dataset with 4 points.
   - Train the same perceptron on this dataset.
   - Evaluate and print the training accuracy.

3. **Comparison and Discussion**
   - Compare the performance of the perceptron on both datasets.
   - Briefly discuss why it succeeds in one case and fails in the other.

**Key Points**:
- Reinforces the perceptron’s ability to solve linearly separable problems.
- Provides a concrete failure case (XOR) that motivates the need for multi-layer networks.
- Sets up the transition to MLPs and non-linear modeling in the next exercise.
---

## Exercise 2 (15 minutes): Building an MLP Using PyTorch or TensorFlow

**Objective**: Build and train a small feedforward neural network to classify the same 2D dataset.

1. **Model Definition**
   - Use a framework (`nn.Sequential` in PyTorch or `Sequential` in Keras).
   - Define: 2 input → 1 hidden layer (ReLU) → 1 output (Sigmoid).
2. **Training Loop**
   - Define loss (`BCELoss` / `binary_crossentropy`) and optimizer (`SGD` or `Adam`).
   - Train for 50–100 epochs and print training loss.
3. **Prediction**
   - Evaluate accuracy on the training set.

**Key Points**:
- Introduces the practical framework usage with a familiar problem.
- Reinforces how weights are updated via backpropagation.

---

## Exercise 3 (10 minutes): Exploring Activation Functions on the Iris Dataset

**Objective**: Compare the effect of different activation functions in the hidden layer.

1. **Try Different Activations**
   - Swap out ReLU for Sigmoid and Tanh in the same model.
2. **Measure Impact**
   - Plot loss over epochs for each version.
   - Compare convergence speed and final accuracy.

**Key Points**:
- Highlights the importance of non-linearity.
- Shows how activation choice affects learning dynamics and gradients.

---

## Exercise 4 (15 minutes): Preventing Overfitting with Dropout and Batch Normalization on the Iris Dataset

**Objective**: Apply dropout and batch norm to improve generalization.

1. **Modify Model**
   - Add `Dropout(0.5)` after the hidden layer.
   - Optionally add `BatchNorm1d` (PyTorch) or `BatchNormalization` (Keras).
2. **Train & Evaluate**
   - Compare training and validation losses.
   - Observe whether overfitting is reduced.

**Key Points**:
- Dropout randomly deactivates neurons to prevent co-adaptation.
- Batch normalization stabilizes and accelerates training.

---

## Exercise 5 (10 minutes): Experimenting with Optimizers

**Objective**: Compare the performance of different optimizers.

1. **Try Optimizers**
   - Run models using `SGD`, `Adam`, and `RMSprop`.
2. **Track Results**
   - Plot training loss for each optimizer.
   - Discuss which optimizer converged fastest or most reliably.

**Key Points**:
- Shows practical implications of optimizer choice.
- Reinforces the idea of adaptive learning rates.

---

## Exercise 6 (15 minutes): Tuning Learning Rate

**Objective**: Explore how different learning rates impact model performance.

1. **Vary Learning Rate**
   - Try values like `0.1`, `0.01`, and `0.001` using the same optimizer.
2. **Visualize Learning**
   - Plot training loss curves for each learning rate.
3. **Discuss**
   - Identify signs of instability, slow convergence, or getting stuck.

**Key Points**:
- Demonstrates trade-offs in speed vs. stability.
- Helps build intuition for choosing hyperparameters.

---

## Wrap-Up (Optional 5 minutes or as you go)
- **Group Discussion:** What affected performance most? What surprised you?
- **Bonus Ideas:** Apply the same model to a real dataset (e.g., `make_moons` or MNIST).
- **Next Steps:** Try deeper networks, CNNs, or hyperparameter tuning with grid/random search.
