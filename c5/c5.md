### **Slide 1: Title & Introduction (5 minutes)**
- **Title:** “Neural Networks – Fundamentals & Training”
- **Subtitle:** Course 5 of the Machine Learning Series
- **Objectives:**
  - Understand the structure and training of neural networks.
  - Build a network using TensorFlow and PyTorch.
  - Learn core concepts: perceptrons, activation functions, backpropagation.
  - Explore optimization, regularization, and practical tips.

---

### **Slide 2: Introduction to Neural Networks (5 minutes)**
- **Definition:** Neural networks are a series of algorithms mimicking the human brain to recognize patterns.
- **Structure:** Composed of layers of connected “neurons” (nodes).
- **Why It Matters:** Foundation for deep learning and AI applications.
- **Common Use Cases:** Image classification, NLP, game playing.

---

### **Slide 3: TensorFlow vs. PyTorch (5 minutes)**
- **TensorFlow:**
  - Static computation graphs.
  - Excellent for deployment and performance.
- **PyTorch:**
  - Dynamic graphs.
  - More intuitive and Pythonic; preferred for research.
- **[An example showing equivalent code for defining a neural net in both TensorFlow and PyTorch]**

---

### **Slide 4: Perceptron – The Simplest Network (10 minutes)**
- **Concept:**
  - Single-layer neural network.
  - Binary classifier that maps inputs to outputs via weights.
- **How It Works:**
  - Weighted sum → activation → prediction.
- **[An example: Using a perceptron to classify points above or below a line]**

---

### **Slide 5: Building Neural Nets from Scratch (5 minutes)**
- **Multi-Layer Perceptron (MLP):**
  - Input layer → hidden layers → output layer.
  - Depth and layer sizes affect model capacity.
- **[An example: Manually implementing an MLP to classify handwritten digits]**

---

### **Slide 6: Backpropagation & Gradient Descent (10 minutes)**
- **Backpropagation:**
  - Computes how errors should adjust weights.
- **Gradient Descent:**
  - Optimization method to minimize loss by updating weights.
- **Variants:** Batch, stochastic, mini-batch.
- **[An example showing how loss decreases during training with gradient descent]**

---

### **Slide 7: The Learning Loop in Practice (5 minutes)**
- **Steps:**
  1. Forward pass to compute output.
  2. Compute loss.
  3. Backpropagation to get gradients.
  4. Optimizer updates weights.
- **[An example: PyTorch training loop on a simple dataset]**

---

### **Slide 8: Activation Functions (10 minutes)**
- **Why Needed:** Introduce non-linearity for complex function mapping.
- **Common Types:**
  - **ReLU:** Efficient, default for most networks.
  - **Sigmoid:** Squashes values, good for probabilities.
  - **Tanh:** Zero-centered, similar to sigmoid.
  - **Softmax:** Multi-class output probabilities.
- **[An example visualizing output of each activation on sample data]**

---

### **Slide 9: Learning Rates, Optimizers & Loss Functions (10 minutes)**
- **Learning Rate:** Controls step size of updates.
- **Optimizers:**
  - SGD, Adam, RMSprop.
  - Impact convergence speed and stability.
- **Loss Functions:**
  - MSE, Cross-Entropy (binary & categorical).
- **[An example comparing different optimizers on a classification task]**

---

### **Slide 10: Overfitting & Regularization (10 minutes)**
- **Overfitting:** Model performs well on training data but poorly on unseen data.
- **Techniques:**
  - **Dropout:** Randomly deactivate neurons during training.
  - **Batch Normalization:** Normalize inputs of each layer.
- **Other Tools:** Early stopping, weight decay.
- **[An example showing training vs validation loss with and without dropout]**

---

### **Slide 11: Summary & Q&A (5 minutes)**
- **Recap Key Concepts:**
  - Neural network structure and training loop.
  - Activation functions and optimizers.
  - Regularization to prevent overfitting.
- **Next Steps:**
  - Practice implementing small networks.
  - Explore convolutional neural networks (CNNs) in the next session.

