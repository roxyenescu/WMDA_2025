## **Exercise 1 (10 minutes): Semantic Similarity with Word Embeddings**

**Objective**: Understand how word embeddings capture word meaning using vector operations.

1. **Define Word Vectors**

   * Use manually created 3D vectors for `king`, `queen`, `man`, and `woman`.
2. **Measure Similarity**

   * Compute cosine similarities between each pair.
3. **Do a Word Analogy**

   * Show `king - man + woman ≈ queen`.

**Key Points**:

* Vectors encode relationships like gender or royalty.
* Arithmetic on embeddings reveals latent structure.

---

## **Exercise 2 (10 minutes): Training Word2Vec on a Toy Corpus**

**Objective**: Train a small Word2Vec model to learn basic word relationships.

1. **Prepare Corpus**

   * Use a short, hand-written set of sentences.
2. **Train Model**

   * Use `gensim.Word2Vec` with `window=2`, `vector_size=10`.
3. **Explore Vectors**

   * Print embeddings, find most similar words.

**Key Points**:

* Word2Vec learns from context windows.
* Even tiny datasets can show useful relationships.

---

## **Exercise 3 (15 minutes): Sentiment Classification with LSTM**

**Objective**: Train an LSTM to classify text sentiment.

1. **Dataset**

   * Use a few manually labeled sentences (positive/negative).
2. **Model**

   * `Embedding` → `LSTM` → `Dense(sigmoid)` using Keras.
3. **Train and Evaluate**

   * Use binary crossentropy, track accuracy.

**Key Points**:

* LSTMs process text sequentially.
* Can capture patterns in sentiment-bearing phrases.

---

## **Exercise 4 (10 minutes): Visualizing Attention Mechanism**

**Objective**: Simulate attention weights and show how they focus on important words.

1. **Mock Encoder States**

   * Generate random hidden states for a 5-word sentence.
2. **Implement Attention**

   * Compute attention weights using a small attention module.
3. **Visualize**

   * Show attention weights as a bar plot.

**Key Points**:

* Attention scores can be interpreted visually.
* Shows model “focus” over a sentence.

---

## **Exercise 5 (10 minutes): Self-Attention Heatmap for a Sentence**

**Objective**: Illustrate how each word in a sentence attends to every other word.

1. **Simulate Self-Attention**

   * Create random attention scores for 5-token input.
2. **Visualize with `matplotlib`**

   * Create a heatmap of attention weights.
3. **Label Words**

   * Use a fixed toy sentence like `\"The cat sat on mat\"`.

**Key Points**:

* Self-attention powers Transformers.
* Each token sees the full context.

---

## **Exercise 6 (10 minutes): BERT vs. GPT for Language Completion**

**Objective**: Compare masked word prediction (BERT) vs. prompt continuation (GPT).

1. **Use HuggingFace Pipelines**

   * `fill-mask` for BERT, `text-generation` for GPT.
2. **Run Examples**

   * BERT: `"The capital of France is [MASK]."`
   * GPT: `"Once upon a time,"`
3. **Discuss Results**

   * What does each model output?

**Key Points**:

* BERT uses bidirectional context.
* GPT generates text sequentially.

---

## **Exercise 7 (10 minutes): Abstractive Summarization with BART**

**Objective**: Summarize a short paragraph using a pre-trained transformer.

1. **Input Text**

   * Use a few sentences describing a news event or story.
2. **Generate Summary**

   * Use `facebook/bart-large-cnn` with `pipeline(\"summarization\")`.
3. **Compare**

   * Show original and summarized versions.

**Key Points**:

* Pre-trained models like BART can paraphrase and condense.
* Summarization is an example of sequence-to-sequence generation.

---

## **Wrap-Up (5 minutes)**

* **Discuss**: Which model surprised you the most? What seemed most intuitive?
* **Review Concepts**:

  * Word vectors & analogies
  * LSTMs vs. attention
  * Transformer-based generation and summarization
* **Next Steps**:

  * Try fine-tuning a pre-trained model.
  * Experiment with a real dataset (IMDb, CNN/DM, etc.).
