## Exercise 1 (10 minutes): Set Up & Naïve Bayes Classification

1. **Objective**
   - Load a small classification dataset from scikit-learn (e.g., the Iris or Wine dataset).
   - Implement a Naïve Bayes classifier.

2. **Instructions**
   1. Import the necessary libraries (`pandas`, `numpy`, `sklearn`).
   2. Load the dataset using `sklearn.datasets.load_wine()` (or another dataset).
   3. Split the dataset into training and testing sets (use an 80/20 split).
   4. Train a Naïve Bayes classifier (`sklearn.naive_bayes.GaussianNB`).
   5. Predict on the test set and print out the accuracy score.

3. **Key Points to Check**
   - Are you splitting the data correctly?
   - Is the model training without errors?
   - Do you observe a reasonable accuracy?

4. **Possible Extension** (if time remains)
   - Print and interpret the confusion matrix for deeper insight into the performance.

---

## Exercise 2 (10 minutes): Logistic Regression & Model Comparison

1. **Objective**
   - Implement Logistic Regression on the same dataset from Exercise 1.
   - Compare performance with Naïve Bayes.

2. **Instructions**
   1. Reuse your dataset and train/test split from Exercise 1.
   2. Train a Logistic Regression model (`sklearn.linear_model.LogisticRegression`).
   3. Compute the accuracy, precision, and recall.
   4. Compare these metrics with the results from Naïve Bayes.

3. **Key Points to Check**
   - Notice whether Logistic Regression outperforms Naïve Bayes or vice versa (and think about why).
   - Inspect any warnings about convergence in Logistic Regression (you might need to tweak `max_iter`).

---

## Exercise 3 (10 minutes): Classification Trees

1. **Objective**
   - Implement a Decision Tree Classifier and visualize or interpret its structure.

2. **Instructions**
   1. Use the same dataset split (or create a new split if you prefer).
   2. Train a decision tree (`sklearn.tree.DecisionTreeClassifier`).
   3. Check the accuracy on the test set.
   4. Use `sklearn.tree.plot_tree` or `export_graphviz` (if available) to visualize the tree structure (or at least examine `feature_importances_` to see which features matter most).

3. **Key Points to Check**
   - Watch for overfitting (especially if max depth isn’t restricted).
   - Observe how many nodes/leaves get created.

4. **Possible Extension** (if time remains)
   - Try limiting the max depth of the tree and see if it affects performance.

---

## Exercise 4 (10 minutes): Spam Classification with Scikit-learn

1. **Objective**
   - Apply your classification skills to a realistic spam-versus-not-spam dataset.
   - Use the spambase dataset (it can be found in many places e.g. https://archive.ics.uci.edu/dataset/94/spambase)

2. **Instructions**
   1. Load a spam dataset if available (e.g., the public “SMS Spam Collection” or a similar text dataset). Otherwise, use any text-based dataset that you have access to.
   2. Convert the text into numerical features (e.g., using `CountVectorizer` or `TfidfVectorizer`).
   3. Split the data into training and testing sets.
   4. Train any of the three classifiers (Naïve Bayes, Logistic Regression, or Decision Tree) to predict spam vs. not spam.
   5. Print the accuracy and confusion matrix.

3. **Key Points to Check**
   - Ensure you are properly vectorizing text data (removing stopwords if desired).
   - Compare performance across different classifiers if time allows.

4. **Possible Extension** (if time remains)
   - Evaluate precision and recall specifically for the spam class (positive class).

---

## Exercise 5 (10 minutes): Sentiment Analysis on Movie Reviews

1. **Objective**
   - Perform sentiment classification on a small set of movie reviews (positive vs. negative).

2. **Instructions**
   1. If you have the dataset, load a small subset of movie reviews labeled “positive” or “negative.” (Alternatively, use a built-in dataset like `nltk.corpus.movie_reviews`, if accessible.)
   2. Again, convert text to numerical features (`TfidfVectorizer` is often better for sentiment tasks).
   3. Train a Logistic Regression model for simplicity (you can choose another classifier if you prefer).
   4. Evaluate the model’s accuracy and F1-score.

3. **Key Points to Check**
   - Are certain words heavily influencing the classification? (Optional: check `coef_` in Logistic Regression.)
   - Class distribution: is it balanced?

4. **Possible Extension** (if time remains)
   - Compute and compare the results with a Decision Tree or Naïve Bayes to see if one performs better on text data.

---

## Exercise 6 (10 minutes): Cross-Validation & Hyperparameter Tuning

1. **Objective**
   - Use cross-validation and a simple hyperparameter search to improve model performance.

2. **Instructions**
   1. Choose one of your previous classification tasks (spam or sentiment analysis recommended).
   2. Use `GridSearchCV` or `RandomizedSearchCV` to tune hyperparameters (e.g., `alpha` for Naïve Bayes, `C` for Logistic Regression, or `max_depth` for Decision Trees).
   3. Run a cross-validation search to find the best parameter values.
   4. Print the best parameters and the best cross-validation score.

3. **Key Points to Check**
   - Make sure you set up the parameter grid or distribution correctly.
   - Keep an eye on runtime; small grids/distributions are recommended for a 10-minute exercise.

4. **Possible Extension** (if time remains)
   - Compare how each classifier’s best model performs on the same cross-validation folds.
   - Experiment with additional metrics like recall or F1-score in the grid search scoring.
